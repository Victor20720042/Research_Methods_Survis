const generatedBibEntries = {
    "10.1145/3627106.3627196": {
        "abstract": "Users seek security \\& privacy (S&P) advice from online resources, including trusted websites and content-sharing platforms. These resources help users understand S&P technologies and tools and suggest actionable strategies. Large Language Models (LLMs) have recently emerged as trusted information sources. However, their accuracy and correctness have been called into question. Prior research has outlined the shortcomings of LLMs in answering multiple-choice questions and user ability to inadvertently circumvent model restrictions (e.g., to produce toxic content). Yet, the ability of LLMs to provide reliable S&P advice is not well-explored. In this paper, we measure their ability to refute popular S&P misconceptions that the general public holds. We first study recent academic literature to curate a dataset of over a hundred S&P-related misconceptions across six different topics. We then query two popular LLMs (Bard and ChatGPT) and develop a labeling guide to evaluate their responses to these misconceptions. To comprehensively evaluate their responses, we further apply three strategies: query each misconception multiple times, generate and query their paraphrases, and solicit source URLs of the responses. Both models demonstrate, on average, a 21.3\\% non-negligible error rate, incorrectly supporting popular S&P misconceptions. The error rate increases to 32.6\\% when we repeatedly query LLMs with the same or paraphrased misconceptions. We also expose that models may partially support a misconception or remain noncommittal, refusing a firm stance on misconceptions. Our exploration of information sources for responses revealed that LLMs are susceptible to providing invalid URLs ( for Bard and for ChatGPT) or point to unrelated sources ( returned by Bard and by ChatGPT). Our findings highlight that existing LLMs are not completely reliable for S&P advice and motivate future work in understanding how users can better interact with this technology.",
        "address": "New York, NY, USA",
        "author": "Chen, Yufan and Arunasalam, Arjun and Celik, Z. Berkay",
        "booktitle": "Proceedings of the 39th Annual Computer Security Applications Conference",
        "doi": "10.1145/3627106.3627196",
        "isbn": "9798400708862",
        "keywords": "type:Evaluate LLMs' Response for Specialized Contexts,Large language models, misconception, security and privacy advice",
        "location": "Austin, TX, USA",
        "numpages": "13",
        "pages": "366-378",
        "publisher": "Association for Computing Machinery",
        "series": "ACSAC '23",
        "title": "Can Large Language Models Provide Security \\& Privacy Advice? Measuring the Ability of LLMs to Refute Misconceptions",
        "type": "inproceedings",
        "url": "https://doi.org/10.1145/3627106.3627196",
        "year": "2023"
    },
    "10.1162/tacl_a_00615": {
        "abstract": "Hallucinated translations can severely undermine and raise safety issues when machine translation systems are deployed in the wild. Previous research on the topic focused on small bilingual models trained on high-resource languages, leaving a gap in our understanding of hallucinations in multilingual models across diverse translation scenarios. In this work, we fill this gap by conducting a comprehensive analysis\u2014over 100 language pairs across various resource levels and going beyond English-centric directions\u2014on both the M2M neural machine translation (NMT) models and GPT large language models (LLMs). Among several insights, we highlight that models struggle with hallucinations primarily in low-resource directions and when translating out of English, where, critically, they may reveal toxic patterns that can be traced back to the training data. We also find that LLMs produce qualitatively different hallucinations to those of NMT models. Finally, we show that hallucinations are hard to reverse by merely scaling models trained with the same data. However, employing more diverse models, trained on different data or with different procedures, as fallback systems can improve translation quality and virtually eliminate certain pathologies.",
        "author": "Guerreiro, Nuno M. and Alves, Duarte M. and Waldendorf, Jonas and Haddow, Barry and Birch, Alexandra and Colombo, Pierre and Martins, Andr\u00e9 F. T.",
        "doi": "10.1162/tacl_a_00615",
        "eprint": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\\_a\\_00615/2197664/tacl\\_a\\_00615.pdf",
        "issn": "2307-387X",
        "journal": "Transactions of the Association for Computational Linguistics",
        "keywords": "type:Limitations of LLM Performance,Multilingual Translation, Hallucinations, Machine Translation Models, Perturbation, Fallback Systems",
        "month": "12",
        "pages": "1500-1517",
        "title": "Hallucinations in Large Multilingual Translation Models",
        "type": "article",
        "url": "https://doi.org/10.1162/tacl\\_a\\_00615",
        "volume": "11",
        "year": "2023"
    },
    "255292": {
        "abstract": "End users learn defensive security behaviors from a variety of channels, including a plethora of security advice given in online articles. A great deal of effort is devoted to getting users to follow this advice. Surprisingly then, little is known about the quality of this advice: Is it comprehensible? Is it actionable? Is it effective? To answer these questions, we first conduct a large-scale, user-driven measurement study to identify 374 unique recommended behaviors contained within 1,264 documents of online security and privacy advice. Second, we develop and validate measurement approaches for evaluating the quality -- comprehensibility, perceived actionability, and perceived efficacy -- of security advice. Third, we deploy these measurement approaches to evaluate the 374 unique pieces of security advice in a user-study with 1,586 users and 41 professional security experts. Our results suggest a crisis of advice prioritization. The majority of advice is perceived by the most users to be at least somewhat actionable, and somewhat comprehensible. Yet, both users and experts struggle to prioritize this advice. For example, experts perceive 89% of the hundreds of studied behaviors as being effective, and identify 118 of them as being among the \"top 5\" things users should do, leaving end-users on their own to prioritize and take action to protect themselves.",
        "author": "Elissa M. Redmiles and Noel Warford and Amritha Jayanti and Aravind Koneru and Sean Kross and Miraida Morales and Rock Stevens and Michelle L. Mazurek",
        "booktitle": "29th USENIX Security Symposium (USENIX Security 20)",
        "isbn": "978-1-939133-17-5",
        "keywords": "type:Evaluation of S&P Advice on the Web, Security Advice, Actionability, Comprehensibility, Advice Prioritization, User Study",
        "month": "aug",
        "pages": "89--108",
        "publisher": "USENIX Association",
        "title": "A Comprehensive Quality Evaluation of Security and Privacy Advice on the Web",
        "type": "inproceedings",
        "url": "https://www.usenix.org/conference/usenixsecurity20/presentation/redmiles",
        "year": "2020"
    },
    "BIRKUN2024102048": {
        "abstract": "The ability of the cutting-edge large language model-powered chatbots to generate human-like answers to user questions hypothetically could be utilized for providing real-time advice on first aid for witnesses of cardiovascular emergencies. This study aimed to evaluate quality of the chatbot responses to inquiries on help in heart attack. The study simulated interrogation of the new Bing chatbot (Microsoft Corporation, USA) with the \u201cheart attack what to do\u201d prompt coming from 3 countries, the Gambia, India and the USA. The chatbot responses (20 per country) were evaluated for congruence with the International First Aid, Resuscitation, and Education Guidelines 2020 using a checklist. For all user inquiries, the chatbot provided answers containing some guidance on first aid. However, the responses commonly left out some potentially life-saving instructions, for instance to encourage the person to stop physical activity, to take antianginal medication, or to start cardiopulmonary resuscitation for unresponsive abnormally breathing person. Mean percentage of the responses having full congruence with the checklist criteria varied from 7.3 for India to 16.8 for the USA. A quarter of responses for the Gambia and the USA, and 45.0% for India contained superfluous guidelines-inconsistent directives. The chatbot advice on help in heart attack has omissions, inaccuracies and misleading instructions, and therefore the chatbot cannot be recommended as a credible source of information on first aid. Active research and organizational efforts are needed to mitigate the risk of uncontrolled misinformation and establish measures for guaranteeing trustworthiness of the chatbot-mediated counseling.",
        "author": "Alexei A. Birkun and Adhish Gautam",
        "doi": "https://doi.org/10.1016/j.cpcardiol.2023.102048",
        "issn": "0146-2806",
        "journal": "Current Problems in Cardiology",
        "keywords": "type:Evaluate LLMs' Response for Specialized Contexts,Chatbot,First Aid,Heart Attack,Large Language Model,Emergency Medical Services",
        "number": "1, Part A",
        "pages": "102048",
        "title": "Large Language Model-based Chatbot as a Source of Advice on First Aid in Heart Attack",
        "type": "article",
        "url": "https://www.sciencedirect.com/science/article/pii/S0146280623004656",
        "volume": "49",
        "year": "2024"
    },
    "bang2023multitaskmultilingualmultimodalevaluation": {
        "abstract": "This paper proposes a framework for quantitatively evaluating interactive LLMs such as ChatGPT using publicly available data sets. We carry out an extensive technical evaluation of ChatGPT using 23 data sets covering 8 different common NLP application tasks. We evaluate the multitask, multilingual and multi-modal aspects of ChatGPT based on these data sets and a newly designed multimodal dataset. We find that ChatGPT outperforms LLMs with zero-shot learning on most tasks and even outperforms fine-tuned models on some tasks. We find that it is better at understanding non-Latin script languages than generating them. It is able to generate multimodal content from textual prompts, via an intermediate code generation step. Moreover, we find that ChatGPT is 63.41% accurate on average in 10 different reasoning categories under logical reasoning, non-textual reasoning, and commonsense reasoning, hence making it an unreliable reasoner. It is, for example, better at deductive than inductive reasoning. ChatGPT suffers from hallucination problems like other LLMs and it generates more extrinsic hallucinations from its parametric memory as it does not have access to an external knowledge base. Finally, the interactive feature of ChatGPT enables human collaboration with the underlying LLM to improve its performance, i.e, 8% ROUGE-1 on summarization and 2% ChrF++ on machine translation, in a multi-turn \"prompt engineering\" fashion. We also release codebase for evaluation set extraction. ",
        "archiveprefix": "arXiv",
        "author": "Yejin Bang and Samuel Cahyawijaya and Nayeon Lee and Wenliang Dai and Dan Su and Bryan Wilie and Holy Lovenia and Ziwei Ji and Tiezheng Yu and Willy Chung and Quyet V. Do and Yan Xu and Pascale Fung",
        "eprint": "2302.04023",
        "keywords": "type:Methods of LLMs' Response Evaluation,Multitask Evaluation,Multilingual Ability,Multimodal Ability,Reasoning Ability,Hallucination Problem",
        "primaryclass": "cs.CL",
        "title": "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",
        "type": "misc",
        "url": "https://arxiv.org/abs/2302.04023",
        "year": "2023"
    },
    "lin2022truthfulqameasuringmodelsmimic": {
        "abstract": "We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.",
        "archiveprefix": "arXiv",
        "author": "Stephanie Lin and Jacob Hilton and Owain Evans",
        "eprint": "2109.07958",
        "keywords": "type:Methods of LLMs' Response Evaluation,TruthfulQA,Imitative Falsehoods,Inverse Scaling,Human Evaluation,Automated Metrics",
        "primaryclass": "cs.CL",
        "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
        "type": "misc",
        "url": "https://arxiv.org/abs/2109.07958",
        "year": "2022"
    },
    "ren2024investigatingfactualknowledgeboundary": {
        "abstract": "Large language models (LLMs) have shown impressive prowess in solving a wide range of tasks with world knowledge. However, it remains unclear how well LLMs are able to perceive their factual knowledge boundaries, particularly under retrieval augmentation settings. In this study, we present the first analysis on the factual knowledge boundaries of LLMs and how retrieval augmentation affects LLMs on open-domain question answering (QA), with a bunch of important findings. Specifically, we focus on three research questions and analyze them by examining QA, priori judgement and posteriori judgement capabilities of LLMs. We show evidence that LLMs possess unwavering confidence in their knowledge and cannot handle the conflict between internal and external knowledge well. Furthermore, retrieval augmentation proves to be an effective approach in enhancing LLMs' awareness of knowledge boundaries. We further conduct thorough experiments to examine how different factors affect LLMs and propose a simple method to dynamically utilize supporting documents with our judgement strategy. Additionally, we find that the relevance between the supporting documents and the questions significantly impacts LLMs' QA and judgemental capabilities.",
        "archiveprefix": "arXiv",
        "author": "Ruiyang Ren and Yuhao Wang and Yingqi Qu and Wayne Xin Zhao and Jing Liu and Hao Tian and Hua Wu and Ji-Rong Wen and Haifeng Wang",
        "eprint": "2307.11019",
        "keywords": "type:Limitations of LLM Performance,Large Language Models, Retrieval Augmentation, Factual Knowledge Boundaries, Open-domain QA, Supporting Documents",
        "primaryclass": "cs.CL",
        "title": "Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation",
        "type": "misc",
        "url": "https://arxiv.org/abs/2307.11019",
        "year": "2024"
    },
    "shen2023chatgpttrustmeasuringcharacterizing": {
        "abstract": "The way users acquire information is undergoing a paradigm shift with the advent of ChatGPT. Unlike conventional search engines, ChatGPT retrieves knowledge from the model itself and generates answers for users. ChatGPT's impressive question-answering (QA) capability has attracted more than 100 million users within a short period of time but has also raised concerns regarding its reliability. In this paper, we perform the first large-scale measurement of ChatGPT's reliability in the generic QA scenario with a carefully curated set of 5,695 questions across ten datasets and eight domains. We find that ChatGPT's reliability varies across different domains, especially underperforming in law and science questions. We also demonstrate that system roles, originally designed by OpenAI to allow users to steer ChatGPT's behavior, can impact ChatGPT's reliability in an imperceptible way. We further show that ChatGPT is vulnerable to adversarial examples, and even a single character change can negatively affect its reliability in certain cases. We believe that our study provides valuable insights into ChatGPT's reliability and underscores the need for strengthening the reliability and security of large language models (LLMs). ",
        "archiveprefix": "arXiv",
        "author": "Xinyue Shen and Zeyuan Chen and Michael Backes and Yang Zhang",
        "eprint": "2304.08979",
        "keywords": "type:Methods of LLMs' Response Evaluation,ChatGPT, Reliability, Adversarial Examples, System Roles, Question-Answering",
        "primaryclass": "cs.CR",
        "title": "In ChatGPT We Trust? Measuring and Characterizing the Reliability of ChatGPT",
        "type": "misc",
        "url": "https://arxiv.org/abs/2304.08979",
        "year": "2023"
    },
    "zhang2023sirenssongaiocean": {
        "abstract": "While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.",
        "archiveprefix": "arXiv",
        "author": "Yue Zhang and Yafu Li and Leyang Cui and Deng Cai and Lemao Liu and Tingchen Fu and Xinting Huang and Enbo Zhao and Yu Zhang and Yulong Chen and Longyue Wang and Anh Tuan Luu and Wei Bi and Freda Shi and Shuming Shi",
        "eprint": "2309.01219",
        "keywords": "type:Limitations of LLM Performance,Hallucination, Large Language Models, Mitigation, Fact-Conflicting, Generation-Time",
        "primaryclass": "cs.CL",
        "title": "Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models",
        "type": "misc",
        "url": "https://arxiv.org/abs/2309.01219",
        "year": "2023"
    },
    "zuccon2023drchatgpttelli": {
        "abstract": "Generative pre-trained language models (GPLMs) like ChatGPT encode in the model's parameters knowledge the models observe during the pre-training phase. This knowledge is then used at inference to address the task specified by the user in their prompt. For example, for the question-answering task, the GPLMs leverage the knowledge and linguistic patterns learned at training to produce an answer to a user question. Aside from the knowledge encoded in the model itself, answers produced by GPLMs can also leverage knowledge provided in the prompts. For example, a GPLM can be integrated into a retrieve-then-generate paradigm where a search engine is used to retrieve documents relevant to the question; the content of the documents is then transferred to the GPLM via the prompt. In this paper we study the differences in answer correctness generated by ChatGPT when leveraging the model's knowledge alone vs. in combination with the prompt knowledge. We study this in the context of consumers seeking health advice from the model. Aside from measuring the effectiveness of ChatGPT in this context, we show that the knowledge passed in the prompt can overturn the knowledge encoded in the model and this is, in our experiments, to the detriment of answer correctness. This work has important implications for the development of more robust and transparent question-answering systems based on generative pre-trained language models.",
        "archiveprefix": "arXiv",
        "author": "Guido Zuccon and Bevan Koopman",
        "eprint": "2302.13793",
        "keywords": "type:Evaluate LLMs' Response for Specialized Contexts,Model selection, Dense retrievers",
        "primaryclass": "cs.CL",
        "title": "Dr ChatGPT, tell me what I want to hear: How prompt knowledge impacts health answer correctness",
        "type": "misc",
        "url": "https://arxiv.org/abs/2302.13793",
        "year": "2023"
    }
};